{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNAqzYZRv0W8",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install numpy pandas scikit-learn matplotlib pyfhel\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "import json\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from Pyfhel import Pyfhel, PyCtxt, PyPtxt"
      ],
      "metadata": {
        "id": "JRtFVV2kv5WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title syntetic generation\n",
        "#generate synthetic CERT-like user-behavior data\n",
        "\n",
        "def generate_synthetic_cert_data(num_users=1000, days=90, insider_frac=0.10, seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    users = [f\"user_{i:03d}\" for i in range(num_users)]\n",
        "    start_date = pd.to_datetime(\"2023-01-01\")\n",
        "    rows = []\n",
        "    insiders = set(random.sample(users, max(1, int(num_users * insider_frac))))\n",
        "\n",
        "    for u in users:\n",
        "        for d in range(days):\n",
        "            date = (start_date + pd.Timedelta(days=d)).strftime(\"%Y-%m-%d\")\n",
        "            #daily logons\n",
        "            logon = np.random.poisson(3)\n",
        "            file_access = np.random.poisson(20)\n",
        "            email_sent = np.random.poisson(5)\n",
        "            web_visits = np.random.poisson(30)\n",
        "            external_transfer = np.random.binomial(1, 0.01)\n",
        "\n",
        "            label = 0\n",
        "            #positive label - elevated behavior\n",
        "            if u in insiders and random.random() < 0.2:\n",
        "                label = 1\n",
        "\n",
        "            if u in insiders and random.random() < 0.05:\n",
        "                file_access += np.random.poisson(50)\n",
        "                external_transfer = 1 if random.random() < 0.3 else external_transfer\n",
        "                email_sent += np.random.poisson(8)\n",
        "\n",
        "            rows.append({\n",
        "                \"user\": u,\n",
        "                \"date\": date,\n",
        "                \"logon\": int(logon),\n",
        "                \"file_access\": int(file_access),\n",
        "                \"email_sent\": int(email_sent),\n",
        "                \"web_visits\": int(web_visits),\n",
        "                \"external_transfer\": int(external_transfer),\n",
        "                \"label\": int(label)\n",
        "            })\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df\n",
        "\n",
        "#larger sample size\n",
        "df = generate_synthetic_cert_data(num_users=1000, days=90, insider_frac=0.10)\n",
        "print(\"Rows:\", len(df))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "usJeMW-Av9_G",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#agregate features\n",
        "\n",
        "FEATURES = ['logon', 'file_access', 'email_sent', 'web_visits', 'external_transfer']\n",
        "TARGET = 'label'\n",
        "\n",
        "X = df[FEATURES].astype(float)\n",
        "y = df[TARGET].astype(int)\n",
        "\n",
        "#standard scaling for ckks\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "#train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "print(\"train shape:\", X_train.shape, \"test shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "cHdZQaKzwIXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train baseline logreg and random forest\n",
        "lr = LogisticRegression(max_iter=1000, random_state=42).fit(X_train, y_train)\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42).fit(X_train, y_train)\n",
        "\n",
        "#eval test set\n",
        "def eval_model(model, X_t, y_t, name=\"model\"):\n",
        "    preds = model.predict(X_t)\n",
        "    probs = model.predict_proba(X_t)[:,1] if hasattr(model, \"predict_proba\") else None\n",
        "    print(name)\n",
        "    print(classification_report(y_t, preds, digits=4))\n",
        "    if probs is not None:\n",
        "        print(\"ROC AUC:\", roc_auc_score(y_t, probs))\n",
        "    print(\"accuracy:\", accuracy_score(y_t, preds))\n",
        "\n",
        "eval_model(lr, X_test, y_test, \"logistic regression\")\n",
        "eval_model(rf, X_test, y_test, \"random forest\")\n"
      ],
      "metadata": {
        "id": "D2mqmw1ZwN9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#HEWrapper\n",
        "\n",
        "class HEContextConfig:\n",
        "    scheme: str = 'CKKS'\n",
        "    n: int = 2**13\n",
        "    scale: float = 2**40\n",
        "    qi_sizes: Tuple[int, ...] = (60, 40, 40, 60)\n",
        "\n",
        "class HEWrapper:\n",
        "    def __init__(self, config: HEContextConfig = HEContextConfig()):\n",
        "        self.config = config\n",
        "        self.HE = Pyfhel()\n",
        "        self.HE.contextGen(\n",
        "            scheme='CKKS',\n",
        "            n=self.config.n,\n",
        "            scale=self.config.scale,\n",
        "            qi_sizes=list(self.config.qi_sizes)\n",
        "        )\n",
        "        self.HE.keyGen()\n",
        "        self.HE.relinKeyGen()\n",
        "        print(f\"Pyfhel CKKS context (n={self.config.n}, scale={self.config.scale}, qi_sizes={self.config.qi_sizes})\")\n",
        "\n",
        "\n",
        "    def encrypt(self, val: float):\n",
        "        return self.HE.encryptFrac(np.array([val], dtype=np.float64))\n",
        "        #encrypt scalar vector\n",
        "\n",
        "    def decrypt(self, ctxt):\n",
        "        plain = self.HE.decryptFrac(ctxt)\n",
        "        return float(plain[0])\n",
        "\n",
        "    def add(self, ct_a, ct_b):\n",
        "        return ct_a + ct_b\n",
        "\n",
        "    def mul_plain(self, ct, scalar: float):\n",
        "        ptxt = self.HE.encodeFrac(np.array([scalar], dtype=np.float64))\n",
        "        res = ct * ptxt\n",
        "        self.HE.rescale_to_next(res)\n",
        "        return res\n",
        "        #multiply ciphertext by plaintext\n",
        "\n",
        "    def sum_ciphertexts(self, list_ct):\n",
        "        if not list_ct:\n",
        "            return self.encrypt(0.0)\n",
        "        acc = list_ct[0]\n",
        "        for ct in list_ct[1:]:\n",
        "            acc = self.add(acc, ct)\n",
        "        return acc\n",
        "\n",
        "he = HEWrapper()"
      ],
      "metadata": {
        "id": "q1yN-mvQwQR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = lr.coef_.flatten().tolist()\n",
        "intercept = float(lr.intercept_[0])\n",
        "feature_names = FEATURES\n",
        "\n",
        "print(\"model weights\")\n",
        "for f, w in zip(feature_names, weights):\n",
        "    print(f, \":\", w)\n",
        "print(\"intercept:\", intercept)\n"
      ],
      "metadata": {
        "id": "RqUplmdBwSgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encrypted scoring small sample set\n",
        "\n",
        "def encrypted_score_sample(he_ctx: HEWrapper, x_raw: np.ndarray, weights: List[float], intercept: float):\n",
        "    ctxts = [he_ctx.encrypt(float(val)) for val in x_raw.tolist()]\n",
        "\n",
        "    #multiply encrypted features by plaintext weight\n",
        "    weighted_cts = [he_ctx.mul_plain(ct, w) for ct, w in zip(ctxts, weights)]\n",
        "\n",
        "    #sum weighted ciphertexts\n",
        "    sum_ct = he_ctx.sum_ciphertexts(weighted_cts)\n",
        "\n",
        "    #add intercept\n",
        "    intercept_ct = he_ctx.encrypt(float(intercept))\n",
        "    he_ctx.HE.mod_switch_to_next(intercept_ct)\n",
        "\n",
        "    total_ct = he_ctx.add(sum_ct, intercept_ct)\n",
        "\n",
        "    #decrypt total\n",
        "    score = he_ctx.decrypt(total_ct)\n",
        "    return score\n",
        "\n",
        "#test random test samples and compare with plaintext lr decision function\n",
        "n_check = 10\n",
        "indices = np.random.choice(len(X_test), size=n_check, replace=False)\n",
        "plaintext_scores = []\n",
        "encrypted_scores = []\n",
        "times = []\n",
        "for idx in indices:\n",
        "    x_sample = X_test[idx]\n",
        "    start = time.perf_counter()\n",
        "    score_enc = encrypted_score_sample(he, x_sample, weights, intercept)\n",
        "    t = time.perf_counter() - start\n",
        "    score_plain = lr.decision_function([x_sample])[0]\n",
        "    plaintext_scores.append(float(score_plain))\n",
        "    encrypted_scores.append(float(score_enc))\n",
        "    times.append(t)\n",
        "\n",
        "df_scores = pd.DataFrame({\n",
        "    \"idx\": indices,\n",
        "    \"plaintext_score\": plaintext_scores,\n",
        "    \"encrypted_decrypted_score\": encrypted_scores,\n",
        "    \"time_sec\": times\n",
        "})\n",
        "df_scores"
      ],
      "metadata": {
        "id": "4Cb4pxVLwV5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predict from decrypted encrypted scores, apply sigmoid in plaintext\n",
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "df_scores['plaintext_prob'] = sigmoid(df_scores['plaintext_score'])\n",
        "df_scores['enc_prob'] = sigmoid(df_scores['encrypted_decrypted_score'])\n",
        "\n",
        "#show comparison\n",
        "display(df_scores[['idx','plaintext_prob','enc_prob','time_sec']].round(6))\n",
        "\n",
        "print(\"mean absolute difference in probabilities:\", np.mean(np.abs(df_scores['plaintext_prob'] - df_scores['enc_prob'])))\n",
        "print(\"mean HE inference time:\", np.mean(df_scores['time_sec']))\n"
      ],
      "metadata": {
        "id": "Nc-NNLXpwaFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#inference on small batch\n",
        "batch_n = min(200, len(X_test))\n",
        "start_total = time.perf_counter()\n",
        "enc_preds = []\n",
        "for i in range(batch_n):\n",
        "    score = encrypted_score_sample(he, X_test[i], weights, intercept)\n",
        "    prob = sigmoid(score)\n",
        "    enc_preds.append(1 if prob >= 0.5 else 0)\n",
        "t_total = time.perf_counter() - start_total\n",
        "\n",
        "print(f\"encrypted batch inference on {batch_n} samples took {t_total:.2f} sec (avg {t_total/batch_n:.4f} sec/sample)\")\n",
        "#compare to plaintext logreg predictions on same samples\n",
        "plain_preds = lr.predict(X_test[:batch_n])\n",
        "print(\"sample agreement rate:\", np.mean(np.array(enc_preds) == plain_preds))\n",
        "print(\"encrypted pipeline produced {}/{} predictions matching plaintext model\".format(int(np.sum(np.array(enc_preds) == plain_preds)), batch_n))\n"
      ],
      "metadata": {
        "id": "dtrZ9u3owni1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}